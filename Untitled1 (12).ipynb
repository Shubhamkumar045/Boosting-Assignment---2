{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b230e5-746b-4d40-8b35-6c7cd0882217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Answer--Gradient Boosting Regression is a machine learning technique used for regression\n",
    "tasks. It belongs to the family of boosting algorithms and is a powerful method for \n",
    "building predictive models based on decision trees.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "Initialization: Like other boosting algorithms, Gradient Boosting Regression starts with \n",
    "an initial model, often a simple one like the mean of the target variable.\n",
    "\n",
    "Building Weak Learners (Decision Trees): In each iteration, a decision tree is trained to \n",
    "predict the residuals (the differences between the actual values and the predictions of\n",
    "the current model) of the previous model. These decision trees are typically \n",
    "shallow to prevent overfitting.\n",
    "\n",
    "Gradient Descent Optimization: The name \"gradient\" in Gradient Boosting comes from \n",
    "the optimization process. The algorithm optimizes the loss function by using gradient \n",
    "descent. It minimizes the loss by updating the model parameters\n",
    "(in this case, the predictions of the weak learners) in the direction that reduces the\n",
    "gradient of the loss function.\n",
    "\n",
    "Combining Weak Learners: After each iteration, the predictions of all weak learners are \n",
    "combined to make the final prediction. The final prediction is the sum of the initial model\n",
    "and the predictions of all subsequent weak learners. By iteratively adding weak learners,\n",
    "the model improves its predictive accuracy.\n",
    "\n",
    "Regularization: Gradient Boosting Regression supports various regularization techniques to\n",
    "prevent overfitting, such as controlling the depth of the decision trees, adjusting the\n",
    "learning rate, and adding regularization terms to the loss function.\n",
    "\n",
    "Stopping Criteria: The training process continues until a certain stopping criterion is met,\n",
    "such as reaching a maximum number of iterations, achieving a specified level of performance \n",
    "improvement, or no further improvement on the validation dataset.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "Answer--import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with mean of y\n",
    "        self.prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        # Iterate over the number of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - self.prediction\n",
    "            \n",
    "            # Train a decision tree regressor on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the prediction\n",
    "            self.prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Add the tree to the list of models\n",
    "            self.models.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions using all the weak learners\n",
    "        predictions = np.array([tree.predict(X) for tree in self.models])\n",
    "        return np.sum(predictions, axis=0)\n",
    "        \n",
    "\n",
    "# Define a simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = X[:3], X[3:]\n",
    "y_train, y_test = y[:3], y[3:]\n",
    "\n",
    "# Import a simple DecisionTreeRegressor for this example\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize and fit the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "Answer--from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(gb_regressor, param_distributions=param_grid,\n",
    "                                   n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Answer--In Gradient Boosting, a weak learner refers to a simple predictive model that\n",
    "performs slightly better than random guessing on a given problem. Weak learners are\n",
    "typically simple and constrained models, such as decision trees with shallow depth\n",
    "(e.g., one-level decision trees or decision stumps).\n",
    "\n",
    "The concept of weak learners is fundamental to boosting algorithms like Gradient\n",
    "Boosting. Here are some key characteristics of weak learners in the context of Gradient Boosting:\n",
    "\n",
    "Simplicity: Weak learners are intentionally kept simple to prevent overfitting and \n",
    "to maintain computational efficiency. They are often constrained in complexity, \n",
    "such as limiting the maximum depth of decision trees.\n",
    "\n",
    "Limited Predictive Power: Weak learners have limited predictive power on their \n",
    "own and may perform poorly when applied to the entire dataset. However, when combined\n",
    "with other weak learners through the boosting process, they contribute to the overall\n",
    "predictive performance of the ensemble.\n",
    "\n",
    "Better Than Random Guessing: While weak learners may not be highly accurate individually, \n",
    "they should perform slightly better than random guessing on the task at hand. This allows\n",
    "them to contribute positively to the ensemble learning process.\n",
    "\n",
    "Focus on Residuals: In Gradient Boosting, weak learners are trained sequentially to\n",
    "predict the residuals (the differences between the true labels and the current ensemble predictions) \n",
    "of the previous models. By focusing on the residuals, weak learners can gradually\n",
    "improve the overall model by addressing the remaining errors.\n",
    "\n",
    "Aggregation: Weak learners are combined through a weighted sum or other aggregation\n",
    "methods to produce the final ensemble prediction. The weights assigned to each weak\n",
    "learner reflect their contribution to reducing the overall error of the model.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Answer--The intuition behind the Gradient Boosting algorithm can be understood through\n",
    "the following key concepts:\n",
    "\n",
    "Gradient Descent Optimization:\n",
    "\n",
    "At its core, Gradient Boosting optimizes a loss function by iteratively minimizing the\n",
    "residuals or errors of the model.\n",
    "It uses the principles of gradient descent to update the model's parameters \n",
    "(e.g., predictions of weak learners) in the direction that reduces the gradient of the loss function.\n",
    "Sequential Training of Weak Learners:\n",
    "\n",
    "Gradient Boosting sequentially trains a series of weak learners (typically decision trees)\n",
    "to correct the errors of the previous models.\n",
    "Each weak learner is trained to predict the residuals or errors of the current ensemble,\n",
    "focusing on the instances where the model performs poorly.\n",
    "Gradient-Based Weighting of Residuals:\n",
    "\n",
    "The residuals or errors of the model are computed using the gradient of the loss function\n",
    "with respect to the predictions.\n",
    "Weak learners are trained to minimize these residuals by approximating the negative \n",
    "gradient of the loss function.\n",
    "Aggregation of Weak Learners:\n",
    "\n",
    "The predictions of all weak learners are combined through a weighted sum to produce \n",
    "the final ensemble prediction.\n",
    "The weights assigned to each weak learner reflect their contribution to reducing the \n",
    "overall error of the model.\n",
    "Regularization and Shrinkage:\n",
    "\n",
    "Gradient Boosting incorporates regularization techniques to prevent overfitting and\n",
    "improve the generalization of the model.\n",
    "It uses a shrinkage parameter (learning rate) to control the contribution of each weak \n",
    "learner to the ensemble, reducing the risk of overfitting.\n",
    "Ensemble of Specialized Models:\n",
    "\n",
    "Each weak learner in the ensemble specializes in capturing different patterns or errors in the data.\n",
    "By combining multiple weak learners, Gradient Boosting creates a strong learner that is\n",
    "capable of capturing complex relationships and making accurate predictions.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Answer--The Gradient Boosting algorithm builds an ensemble of weak learners in a \n",
    "sequential manner. Here's a step-by-step overview of how Gradient Boosting constructs the ensemble:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The ensemble starts with an initial prediction, often set as the mean of the target\n",
    "variable for regression tasks or the log-odds for classification tasks.\n",
    "Compute Residuals:\n",
    "\n",
    "The algorithm computes the residuals, which represent the differences between the \n",
    "actual target values and the current predictions of the ensemble.\n",
    "Train Weak Learner:\n",
    "\n",
    "A weak learner (usually a decision tree) is trained to predict the residuals.\n",
    "The goal of the weak learner is to approximate the negative gradient of the loss function \n",
    "with respect to the current predictions of the ensemble.\n",
    "Update Ensemble Prediction:\n",
    "\n",
    "The predictions of the weak learner are added to the current ensemble predictions, with a \n",
    "scaling factor known as the learning rate.\n",
    "The learning rate controls the contribution of each weak learner to the overall ensemble.\n",
    "Iterative Process:\n",
    "\n",
    "Steps 2 to 4 are repeated iteratively for a predefined number of iterations or until a \n",
    "certain stopping criterion is met.\n",
    "At each iteration, the next weak learner focuses on the residuals of the previous ensemble,\n",
    "aiming to reduce the errors further.\n",
    "Combining Weak Learners:\n",
    "\n",
    "The final ensemble prediction is the sum of the initial prediction and the predictions of \n",
    "all weak learners trained during the boosting process.\n",
    "The contributions of weak learners are weighted based on their performance and the learning rate.\n",
    "Regularization:\n",
    "\n",
    "Gradient Boosting typically employs regularization techniques to prevent overfitting.\n",
    "Regularization may include constraints on the depth of decision trees (tree pruning), \n",
    "learning rate adjustment, and early stopping based on performance on a validation set.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "Answer--Constructing the mathematical intuition behind the Gradient Boosting algorithm involves \n",
    "understanding the underlying principles of optimization, gradient descent, and ensemble learning. \n",
    "Here are the key steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "Define a suitable loss function that measures the discrepancy between the model's predictions \n",
    "and the true target values. Common loss functions include mean squared error for regression\n",
    "and cross-entropy loss for classification.\n",
    "Gradient Descent:\n",
    "\n",
    "Understand the concept of gradient descent, which is an optimization algorithm used to\n",
    "minimize the loss function by iteratively updating the model parameters (predictions)\n",
    "in the direction of the negative gradient of the loss function.\n",
    "Residuals:\n",
    "\n",
    "Compute the residuals, which represent the differences between the true target values \n",
    "and the current predictions of the model. Residuals are used as the target for subsequent\n",
    "weak learners in the ensemble.\n",
    "Weak Learners:\n",
    "\n",
    "Choose a weak learner, typically a decision tree with limited depth, to approximate the\n",
    "negative gradient of the loss function with respect to the current predictions of the ensemble.\n",
    "Learning Rate:\n",
    "\n",
    "Introduce a learning rate parameter that controls the step size of the gradient descent \n",
    "updates. A smaller learning rate results in slower convergence but may lead to better\n",
    "generalization.\n",
    "Sequential Training:\n",
    "\n",
    "Train the weak learners sequentially to predict the residuals of the current ensemble. \n",
    "Each weak learner focuses on reducing the errors or residuals of the previous ensemble.\n",
    "Aggregation:\n",
    "\n",
    "Combine the predictions of all weak learners through a weighted sum to produce the final \n",
    "ensemble prediction. The weights assigned to each weak learner reflect their contribution \n",
    "to reducing the overall error of the model.\n",
    "Regularization:\n",
    "\n",
    "Apply regularization techniques to prevent overfitting and improve the generalization of\n",
    "the model. Regularization may include constraints on the complexity of weak learners \n",
    "(e.g., maximum tree depth), learning rate adjustment, and early stopping based on\n",
    "performance on a validation set.\n",
    "Evaluation:\n",
    "\n",
    "Evaluate the performance of the Gradient Boosting model using appropriate evaluation\n",
    "metrics such as mean squared error for regression or accuracy for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
